{
# Interaction arguments
"start_round" : 0,
"num_rounds" : 20,
"datapoint_length" : 512,
"context_proportion" : 0.5,
"student_sample_bsz" : 512,
"student_sample_temperature" : 1.0,
"teacher_sample_bsz" : 512,
"teacher_sample_temperature" : 1.0,

# Training hyperparameters
"n_sft_epochs" : 8,
"n_po_epochs" : 2,

"student_po_bsz" : 16,
"simpo_beta" : 2,
"simpo_gamma_ratio" : 0.5,
"sft_lambda" : 0.2,

"learning_rate" : 0.00005,
"weight_decay" : 0,
"num_training_steps" : 200000,
"num_warmup_steps" : 2000,
"sft_learning_rate" : 0.00005,
"gradient_clip_norm" : 1,
"seed" : -1,

# Model hyperparameters
"student_model" : "gpt2",

# Experiment arguments
"base_folder" : "babylm-interaction-dpo-baseline/experiments",
"experiment_name" : "testing",

"use_wandb" : False,
"wandb_experiment_name" : "testing_new_acc",
"wandb_project_name" : "babylm_interaction_sweeps",

"save_generated_data" : False,
}